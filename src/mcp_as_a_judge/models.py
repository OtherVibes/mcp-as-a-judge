"""
Data models and schemas for MCP as a Judge.

This module contains all Pydantic models used for data validation,
serialization, and API contracts.
"""

from pydantic import BaseModel, Field

from mcp_as_a_judge.models.task_metadata import TaskMetadata
from mcp_as_a_judge.workflow import WorkflowGuidance


class JudgeResponse(BaseModel):
    """Enhanced response model for all judge tool evaluations.

    This standardized response format ensures consistent feedback
    across all evaluation tools and includes task metadata and workflow guidance.
    """

    # Standard judge response fields
    approved: bool = Field(
        description="Whether the plan/code is approved for implementation"
    )
    required_improvements: list[str] = Field(
        default_factory=list,
        description="Specific improvements needed (empty if approved)",
    )
    feedback: str = Field(
        description="Detailed explanation of the decision and recommendations"
    )

    # Enhanced workflow fields
    current_task_metadata: TaskMetadata = Field(
        description="Current state of task metadata after operation"
    )
    workflow_guidance: WorkflowGuidance = Field(
        description="LLM-generated next steps and instructions"
    )


class ObstacleResolutionDecision(BaseModel):
    """Schema for eliciting user decision when agent encounters obstacles.

    Used by the raise_obstacle tool to capture user choices when
    the agent cannot proceed due to blockers or missing information.
    """

    chosen_option: str = Field(
        description="The option the user chooses from the provided list"
    )
    additional_context: str = Field(
        default="",
        description="Any additional context or modifications the user provides",
    )


# Note: RequirementsClarification and ObstacleResolution models have been replaced
# with dynamic model generation in _generate_dynamic_elicitation_model()
# This allows for context-specific elicitation fields generated by LLM


class WorkflowGuidance(BaseModel):
    """Schema for workflow guidance responses.

    Used by the build_workflow tool to provide
    structured guidance on which tools to use next.
    """

    next_tool: str = Field(
        description="The specific MCP tool that should be called next: 'judge_coding_plan', 'judge_code_change', 'raise_obstacle', or 'elicit_missing_requirements'"
    )
    reasoning: str = Field(
        description="Clear explanation of why this tool should be used next"
    )
    preparation_needed: list[str] = Field(
        default_factory=list,
        description="List of things that need to be prepared before calling the recommended tool",
    )
    guidance: str = Field(
        description="Detailed step-by-step guidance for the AI assistant"
    )


class ResearchValidationResponse(BaseModel):
    """Schema for research validation responses.

    Used by the _validate_research_quality function to parse
    LLM responses about research quality and design alignment.
    """

    research_adequate: bool = Field(
        description="Whether the research is comprehensive enough"
    )
    design_based_on_research: bool = Field(
        description="Whether the design is properly based on research"
    )
    issues: list[str] = Field(
        default_factory=list, description="List of specific issues if any"
    )
    feedback: str = Field(
        description="Detailed feedback on research quality and design alignment"
    )


# Database models for conversation history
# ConversationRecord is now defined in db/interface.py using SQLModel
# DatabaseConfig is now defined in constants.py


# Type aliases for better code readability
ToolResponse = JudgeResponse
ElicitationResponse = str


# Prompt variable models for type safety and validation


class JudgeCodingPlanSystemVars(BaseModel):
    """Variables for judge_coding_plan system prompt."""

    response_schema: str = Field(
        description="JSON schema for the expected response format"
    )


class JudgeCodingPlanUserVars(BaseModel):
    """Variables for judge_coding_plan user prompt."""

    user_requirements: str = Field(
        description="The user's requirements for the coding task"
    )
    context: str = Field(description="Additional context about the task")
    plan: str = Field(description="The coding plan to be evaluated")
    design: str = Field(description="The design documentation")
    research: str = Field(description="Research findings and analysis")
    research_urls: list[str] = Field(
        default_factory=list,
        description="URLs from online research (conditional based on task needs)",
    )
    conversation_history: list = Field(
        default_factory=list,
        description="Previous conversation history as JSON array with timestamps",
    )

    # Conditional research fields
    research_required: bool = Field(
        default=False,
        description="Whether external research is required for this task"
    )
    research_scope: str = Field(
        default="none",
        description="Scope of research required (none, light, deep)"
    )
    research_rationale: str = Field(
        default="",
        description="Explanation of why research is or isn't required"
    )

    # Conditional internal research fields
    internal_research_required: bool = Field(
        default=False,
        description="Whether internal codebase research is needed"
    )
    related_code_snippets: list[str] = Field(
        default_factory=list,
        description="Related code snippets from the codebase"
    )

    # Conditional risk assessment fields
    risk_assessment_required: bool = Field(
        default=False,
        description="Whether risk assessment is needed"
    )
    identified_risks: list[str] = Field(
        default_factory=list,
        description="Areas that could be harmed by the changes"
    )
    risk_mitigation_strategies: list[str] = Field(
        default_factory=list,
        description="Strategies to mitigate identified risks"
    )


class JudgeCodeChangeSystemVars(BaseModel):
    """Variables for judge_code_change system prompt."""

    response_schema: str = Field(
        description="JSON schema for the expected response format"
    )


class JudgeCodeChangeUserVars(BaseModel):
    """Variables for judge_code_change user prompt."""

    user_requirements: str = Field(
        description="The user's requirements for the code change"
    )
    file_path: str = Field(description="Path to the file being changed")
    change_description: str = Field(description="Description of what the change does")
    code_change: str = Field(description="The actual code content being reviewed")
    context: str = Field(description="Additional context about the code change")
    conversation_history: list = Field(
        default_factory=list,
        description="Previous conversation history as JSON array with timestamps",
    )


class ResearchValidationSystemVars(BaseModel):
    """Variables for research_validation system prompt."""

    response_schema: str = Field(
        description="JSON schema for the expected response format"
    )


class ResearchValidationUserVars(BaseModel):
    """Variables for research_validation user prompt."""

    user_requirements: str = Field(description="The user's requirements for the task")
    plan: str = Field(description="The proposed plan")
    design: str = Field(description="The design documentation")
    research: str = Field(description="Research findings to be validated")
    research_urls: list[str] = Field(
        default_factory=list,
        description="URLs from MANDATORY online research - minimum 3 URLs required",
    )
    context: str = Field(description="Additional context about the research validation")
    conversation_history: list = Field(
        default_factory=list,
        description="Previous conversation history as JSON array with timestamps",
    )


class WorkflowGuidanceSystemVars(BaseModel):
    """Variables for build_workflow system prompt."""

    response_schema: str = Field(
        description="JSON schema for the expected response format"
    )


class WorkflowGuidanceUserVars(BaseModel):
    """Variables for build_workflow user prompt."""

    task_description: str = Field(description="Description of the development task")
    context: str = Field(description="Additional context about the task")
    conversation_history: list = Field(
        default_factory=list,
        description="Previous conversation history as JSON array with timestamps",
    )


class ValidationErrorSystemVars(BaseModel):
    """Variables for validation_error system prompt."""

    # No additional variables needed for system prompt


class ValidationErrorUserVars(BaseModel):
    """Variables for validation_error user prompt."""

    validation_issue: str = Field(
        description="The specific validation issue that occurred"
    )
    context: str = Field(description="Additional context about the validation failure")


class DynamicSchemaSystemVars(BaseModel):
    """Variables for dynamic_schema system prompt."""

    # No additional variables needed for system prompt


class DynamicSchemaUserVars(BaseModel):
    """Variables for dynamic_schema user prompt."""

    context: str = Field(
        description="Context about what information needs to be gathered"
    )
    information_needed: str = Field(
        description="Specific description of what information is needed from the user"
    )
    current_understanding: str = Field(
        description="What we currently understand about the situation"
    )


class ElicitationFallbackUserVars(BaseModel):
    """Variables for elicitation fallback user prompt template."""

    original_message: str = Field(
        description="The original elicitation message that was intended for the user"
    )
    required_fields: list[str] = Field(
        description="List of required field descriptions for the user to provide"
    )
    optional_fields: list[str] = Field(
        description="List of optional field descriptions for the user to provide"
    )


class TestingEvaluationSystemVars(BaseModel):
    """Variables for testing evaluation system prompt."""

    response_schema: str = Field(
        description="JSON schema for the expected response format"
    )


class TestingEvaluationUserVars(BaseModel):
    """Variables for testing evaluation user prompt."""

    user_requirements: str = Field(
        description="The user's requirements for the coding task"
    )
    task_description: str = Field(
        description="Description of the coding task being tested"
    )
    modified_files: list[str] = Field(
        default_factory=list,
        description="List of implementation files that were modified"
    )
    test_summary: str = Field(
        description="Summary of the testing implementation"
    )
    test_files: list[str] = Field(
        default_factory=list,
        description="List of test files that were created"
    )
    test_execution_results: str = Field(
        description="Results from running the tests"
    )
    test_coverage_report: str = Field(
        description="Test coverage report data"
    )
    test_types_implemented: list[str] = Field(
        default_factory=list,
        description="Types of tests implemented (unit, integration, e2e, etc.)"
    )
    testing_framework: str = Field(
        description="Testing framework used"
    )
    performance_test_results: str = Field(
        description="Performance test results if applicable"
    )
    manual_test_notes: str = Field(
        description="Notes from manual testing if applicable"
    )
    conversation_history: list = Field(
        default_factory=list,
        description="Previous conversation history as JSON array with timestamps"
    )
