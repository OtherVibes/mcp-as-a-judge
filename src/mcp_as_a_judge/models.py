"""
Data models and schemas for MCP as a Judge.

This module contains all Pydantic models used for data validation,
serialization, and API contracts.
"""

from pydantic import BaseModel, Field

from mcp_as_a_judge.models.task_metadata import TaskMetadata
from mcp_as_a_judge.workflow import WorkflowGuidance


class JudgeResponse(BaseModel):
    """Enhanced response model for all judge tool evaluations.

    This standardized response format ensures consistent feedback
    across all evaluation tools and includes task metadata and workflow guidance.
    """

    # Standard judge response fields
    approved: bool = Field(
        description="Whether the plan/code is approved for implementation"
    )
    required_improvements: list[str] = Field(
        default_factory=list,
        description="Specific improvements needed (empty if approved)",
    )
    feedback: str = Field(
        description="Detailed explanation of the decision and recommendations"
    )

    # Enhanced workflow fields
    current_task_metadata: TaskMetadata = Field(
        description="Current state of task metadata after operation"
    )
    workflow_guidance: WorkflowGuidance = Field(
        description="LLM-generated next steps and instructions"
    )


class ObstacleResolutionDecision(BaseModel):
    """Schema for eliciting user decision when agent encounters obstacles.

    Used by the raise_obstacle tool to capture user choices when
    the agent cannot proceed due to blockers or missing information.
    """

    chosen_option: str = Field(
        description="The option the user chooses from the provided list"
    )
    additional_context: str = Field(
        default="",
        description="Any additional context or modifications the user provides",
    )


# Note: RequirementsClarification and ObstacleResolution models have been replaced
# with dynamic model generation in _generate_dynamic_elicitation_model()
# This allows for context-specific elicitation fields generated by LLM


class WorkflowGuidance(BaseModel):
    """Schema for workflow guidance responses.

    Used by the build_workflow tool to provide
    structured guidance on which tools to use next.
    """

    next_tool: str = Field(
        description="The specific MCP tool that should be called next: 'judge_coding_plan', 'judge_code_change', 'raise_obstacle', or 'elicit_missing_requirements'"
    )
    reasoning: str = Field(
        description="Clear explanation of why this tool should be used next"
    )
    preparation_needed: list[str] = Field(
        default_factory=list,
        description="List of things that need to be prepared before calling the recommended tool",
    )
    guidance: str = Field(
        description="Detailed step-by-step guidance for the AI assistant"
    )


class ResearchValidationResponse(BaseModel):
    """Schema for research validation responses.

    Used by the _validate_research_quality function to parse
    LLM responses about research quality and design alignment.
    """

    research_adequate: bool = Field(
        description="Whether the research is comprehensive enough"
    )
    design_based_on_research: bool = Field(
        description="Whether the design is properly based on research"
    )
    issues: list[str] = Field(
        default_factory=list, description="List of specific issues if any"
    )
    feedback: str = Field(
        description="Detailed feedback on research quality and design alignment"
    )


class ResearchComplexityFactors(BaseModel):
    """Analysis factors for determining research complexity."""

    domain_specialization: str = Field(
        description="Level of domain specialization: 'general', 'specialized', 'highly_specialized'"
    )
    technology_maturity: str = Field(
        description="Maturity of required technologies: 'established', 'emerging', 'cutting_edge'"
    )
    integration_scope: str = Field(
        description="Scope of system integration: 'isolated', 'moderate', 'system_wide'"
    )
    existing_solutions: str = Field(
        description="Availability of existing solutions: 'abundant', 'limited', 'scarce'"
    )
    risk_level: str = Field(
        description="Implementation risk level: 'low', 'medium', 'high'"
    )


class ResearchRequirementsAnalysis(BaseModel):
    """LLM analysis of research requirements for a task."""

    expected_url_count: int = Field(
        description="Recommended number of research URLs for optimal coverage",
        ge=0,
        le=10,
    )
    minimum_url_count: int = Field(
        description="Minimum acceptable number of URLs for basic adequacy", ge=0, le=5
    )
    reasoning: str = Field(
        description="Detailed explanation of why these URL counts are appropriate"
    )
    complexity_factors: ResearchComplexityFactors = Field(
        description="Breakdown of complexity analysis factors"
    )
    quality_requirements: list[str] = Field(
        default_factory=list,
        description="Specific requirements for research source quality and types",
    )


class URLValidationResult(BaseModel):
    """Result of validating provided URLs against dynamic requirements."""

    adequate: bool = Field(
        description="Whether the provided URLs meet the dynamic requirements"
    )
    provided_count: int = Field(description="Number of URLs actually provided")
    expected_count: int = Field(description="Expected number of URLs based on analysis")
    minimum_count: int = Field(description="Minimum acceptable number of URLs")
    feedback: str = Field(
        description="Detailed feedback about URL adequacy and suggestions"
    )
    meets_quality_standards: bool = Field(
        description="Whether URLs meet quality requirements beyond just count"
    )


# Database models for conversation history
# ConversationRecord is now defined in db/interface.py using SQLModel
# DatabaseConfig is now defined in constants.py


# Type aliases for better code readability
ToolResponse = JudgeResponse
ElicitationResponse = str


# Prompt variable models for type safety and validation


class JudgeCodingPlanSystemVars(BaseModel):
    """Variables for judge_coding_plan system prompt."""

    response_schema: str = Field(
        description="JSON schema for the expected response format"
    )


class JudgeCodingPlanUserVars(BaseModel):
    """Variables for judge_coding_plan user prompt."""

    user_requirements: str = Field(
        description="The user's requirements for the coding task"
    )
    context: str = Field(description="Additional context about the task")
    plan: str = Field(description="The coding plan to be evaluated")
    design: str = Field(description="The design documentation")
    research: str = Field(description="Research findings and analysis")
    research_urls: list[str] = Field(
        default_factory=list,
        description="URLs from online research (conditional based on task needs)",
    )
    conversation_history: list = Field(
        default_factory=list,
        description="Previous conversation history as JSON array with timestamps",
    )

    # Conditional research fields
    research_required: bool = Field(
        default=False, description="Whether external research is required for this task"
    )
    research_scope: str = Field(
        default="none", description="Scope of research required (none, light, deep)"
    )
    research_rationale: str = Field(
        default="", description="Explanation of why research is or isn't required"
    )

    # Conditional internal research fields
    internal_research_required: bool = Field(
        default=False, description="Whether internal codebase research is needed"
    )
    related_code_snippets: list[str] = Field(
        default_factory=list, description="Related code snippets from the codebase"
    )

    # Conditional risk assessment fields
    risk_assessment_required: bool = Field(
        default=False, description="Whether risk assessment is needed"
    )
    identified_risks: list[str] = Field(
        default_factory=list, description="Areas that could be harmed by the changes"
    )
    risk_mitigation_strategies: list[str] = Field(
        default_factory=list, description="Strategies to mitigate identified risks"
    )

    # Dynamic URL requirements fields - NEW
    expected_url_count: int = Field(
        default=0,
        description="LLM-determined expected number of research URLs for this task",
    )
    minimum_url_count: int = Field(
        default=0, description="LLM-determined minimum acceptable URL count"
    )
    url_requirement_reasoning: str = Field(
        default="", description="LLM explanation of why specific URL count is needed"
    )


class JudgeCodeChangeSystemVars(BaseModel):
    """Variables for judge_code_change system prompt."""

    response_schema: str = Field(
        description="JSON schema for the expected response format"
    )


class JudgeCodeChangeUserVars(BaseModel):
    """Variables for judge_code_change user prompt."""

    user_requirements: str = Field(
        description="The user's requirements for the code change"
    )
    file_path: str = Field(description="Path to the file being changed")
    change_description: str = Field(description="Description of what the change does")
    code_change: str = Field(description="The actual code content being reviewed")
    context: str = Field(description="Additional context about the code change")
    conversation_history: list = Field(
        default_factory=list,
        description="Previous conversation history as JSON array with timestamps",
    )


class ResearchValidationSystemVars(BaseModel):
    """Variables for research_validation system prompt."""

    response_schema: str = Field(
        description="JSON schema for the expected response format"
    )


class ResearchValidationUserVars(BaseModel):
    """Variables for research_validation user prompt."""

    user_requirements: str = Field(description="The user's requirements for the task")
    plan: str = Field(description="The proposed plan")
    design: str = Field(description="The design documentation")
    research: str = Field(description="Research findings to be validated")
    research_urls: list[str] = Field(
        default_factory=list,
        description="URLs from online research - count determined dynamically based on task complexity",
    )
    context: str = Field(description="Additional context about the research validation")
    conversation_history: list = Field(
        default_factory=list,
        description="Previous conversation history as JSON array with timestamps",
    )


class WorkflowGuidanceSystemVars(BaseModel):
    """Variables for build_workflow system prompt."""

    response_schema: str = Field(
        description="JSON schema for the expected response format"
    )


class WorkflowGuidanceUserVars(BaseModel):
    """Variables for workflow_guidance user prompt."""

    task_id: str = Field(description="Task ID")
    task_title: str = Field(description="Task title")
    task_description: str = Field(description="Description of the development task")
    user_requirements: str = Field(description="User requirements for the task")
    current_state: str = Field(description="Current task state")
    state_description: str = Field(description="Description of current state")
    current_operation: str = Field(description="Current operation being performed")
    task_size: str = Field(description="Task size classification")
    task_size_definitions: str = Field(description="Task size definitions")
    state_transitions: str = Field(description="Valid state transitions")
    tool_descriptions: str = Field(description="Available tool descriptions")
    conversation_context: str = Field(description="Conversation history context")
    operation_context: str = Field(description="Current operation context")
    response_schema: str = Field(
        description="JSON schema for the expected response format"
    )


class ValidationErrorSystemVars(BaseModel):
    """Variables for validation_error system prompt."""

    # No additional variables needed for system prompt


class ValidationErrorUserVars(BaseModel):
    """Variables for validation_error user prompt."""

    validation_issue: str = Field(
        description="The specific validation issue that occurred"
    )
    context: str = Field(description="Additional context about the validation failure")


class DynamicSchemaSystemVars(BaseModel):
    """Variables for dynamic_schema system prompt."""

    # No additional variables needed for system prompt


class DynamicSchemaUserVars(BaseModel):
    """Variables for dynamic_schema user prompt."""

    context: str = Field(
        description="Context about what information needs to be gathered"
    )
    information_needed: str = Field(
        description="Specific description of what information is needed from the user"
    )
    current_understanding: str = Field(
        description="What we currently understand about the situation"
    )


class ElicitationFallbackUserVars(BaseModel):
    """Variables for elicitation fallback user prompt template."""

    original_message: str = Field(
        description="The original elicitation message that was intended for the user"
    )
    required_fields: list[str] = Field(
        description="List of required field descriptions for the user to provide"
    )
    optional_fields: list[str] = Field(
        description="List of optional field descriptions for the user to provide"
    )


class ResearchRequirementsAnalysisSystemVars(BaseModel):
    """Variables for research_requirements_analysis system prompt."""

    response_schema: str = Field(
        description="JSON schema for the expected response format"
    )


class ResearchRequirementsAnalysisUserVars(BaseModel):
    """Variables for research_requirements_analysis user prompt."""

    task_title: str = Field(description="Title of the coding task")
    task_description: str = Field(description="Detailed description of the task")
    user_requirements: str = Field(description="User requirements for the task")
    research_scope: str = Field(description="Current research scope (none/light/deep)")
    research_rationale: str = Field(
        description="Rationale for why research is needed at current scope"
    )
    context: str = Field(description="Additional context about the task and project")


class TestingEvaluationSystemVars(BaseModel):
    """Variables for testing evaluation system prompt."""

    response_schema: str = Field(
        description="JSON schema for the expected response format"
    )


class TestingEvaluationUserVars(BaseModel):
    """Variables for testing evaluation user prompt."""

    user_requirements: str = Field(
        description="The user's requirements for the coding task"
    )
    task_description: str = Field(
        description="Description of the coding task being tested"
    )
    modified_files: list[str] = Field(
        default_factory=list,
        description="List of implementation files that were modified",
    )
    test_summary: str = Field(description="Summary of the testing implementation")
    test_files: list[str] = Field(
        default_factory=list, description="List of test files that were created"
    )
    test_execution_results: str = Field(description="Results from running the tests")
    test_coverage_report: str = Field(description="Test coverage report data")
    test_types_implemented: list[str] = Field(
        default_factory=list,
        description="Types of tests implemented (unit, integration, e2e, etc.)",
    )
    testing_framework: str = Field(description="Testing framework used")
    performance_test_results: str = Field(
        description="Performance test results if applicable"
    )
    manual_test_notes: str = Field(
        description="Notes from manual testing if applicable"
    )
    conversation_history: list = Field(
        default_factory=list,
        description="Previous conversation history as JSON array with timestamps",
    )
